[{"id":0,"href":"/contributing/","title":"Contributing","parent":"Google Applied ML","content":" Prerequisites Prior to contributing to a Google Open Source repository, Google, LLC requires non-Google contributors to sign and file a Contributor License Agreement. If you DO NOT have a CLA on file your code WILL NOT be merged into this repository.\nWhy do we have a CLA? Our CLA allows open source projects adminstered by Google to safely accept code and documentation from external contributors.\nFor additional information, please visit our Alphabet CLA Policy and Rationale page.\nIntroduction flowchart LR A(Create GitHub Account) --\u003e|Request access| B(Fork Repository) B --\u003e|Open Clone| C(Checkout Source Code) C --\u003e D(Make Changes) D --\u003e E(Write Tests) E --\u003e F(Test) F --\u003e G(Verify) G --\u003e H(Commit) H --\u003e I(Create PR) I --\u003e J[Collaborate] J --\u003e K{Pass} K --\u003e L[Merge] K --\u003e|Fix Suggestions| C L --\u003e C Steps Ensure you have a GitHub account. Request access if the repository is private. Fork the repository. This makes a copy to your local GitHub account. Clone the newly created fork to your developer machine. git clone \u0026lt;repository name\u0026gt; Make any changes or additions to the code. Write automated test cases. Verify all tests are passing and all code is commented and meets the style guide requirements. _ bazel test //... runs all tests _ bazel coverage //java/... runs test coverage on a specific target. Commit your code, and push back to your fork. git commit add . git commit -a Add comments, if you are referencing a feature or bug, please indicate the number first. git push Verify the build runs in GitHub actions on your own branch. This is important, especially if you are updating BUILD or WORKSPACE files. From the GitHub interface, create a pull request and ensure you comment the reasons and thought processes behind the change or additions. * Understanding Pull Requests Once created, you will collaborate with the maintainers for QA purposes, unless they simply accept the changed and merge it. If not, you may be asked to make some changes and create a new PR. Project Structure The Google Retail Cloud project is divided into the following directory structure:\nconf - Configuration files docs - The documentation site examples - Examples on how to use the python modules google - Is the root library for all google libraries third-party - Is all third party data and licenses. Licensing This project is licensed under the Apache 2.0 license. Please see the LICENSE file in the root of the project.\n"},{"id":1,"href":"/contributing/contributors/","title":"Contributors","parent":"Contributing","content":" Thank you No Open Source Software (OSS) stands alone. The following contributuors have made material contributions to this project and their work is sincerly appreciated.\nName Start Date Bio Abhishek Bhagwat 10/01/2022 Linked In Michael Chi 02/20/2024 Ryan McGuinness 12/01/2023 Linked In "},{"id":2,"href":"/contributing/code_style_guide/","title":"Code Style Guide","parent":"Contributing","content":" HTML / CSS Java Script Java Code Style Guide Go Lang "},{"id":3,"href":"/contributing/code_of_conduct/","title":"Code of Conduct","parent":"Contributing","content":" At Google, we recognize and celebrate the creativity and collaboration of open source contributors and the diversity of skills, experiences, cultures, and opinions they bring to the projects and communities they participate in.\nEvery one of Google\u0026rsquo;s open source projects and communities are inclusive environments, based on treating all individuals respectfully, regardless of gender identity and expression, sexual orientation, disabilities, neurodiversity, physical appearance, body size, ethnicity, nationality, race, age, religion, or similar personal characteristic.\nWe value diverse opinions, but we value respectful behavior more.\nRespectful behavior includes:\nBeing considerate, kind, constructive, and helpful. Not engaging in demeaning, discriminatory, harassing, hateful, sexualized, or physically threatening behavior, speech, and imagery. Not engaging in unwanted physical contact. Some Google open source projects may adopt an explicit project code of conduct, which may have additional detailed expectations for participants. Most of those projects will use our modified Contributor Covenant.\nResolve peacefully We do not believe that all conflict is necessarily bad; healthy debate and disagreement often yields positive results. However, it is never okay to be disrespectful.\nIf you see someone behaving disrespectfully, you are encouraged to address the behavior directly with those involved. Many issues can be resolved quickly and easily, and this gives people more control over the outcome of their dispute. If you are unable to resolve the matter for any reason, or if the behavior is threatening or harassing, report it. We are dedicated to providing an environment where participants feel welcome and safe.\nReporting problems Some Google open source projects may adopt a project-specific code of conduct. In those cases, a Google employee will be identified as the Project Steward, who will receive and handle reports of code of conduct violations. In the event that a project hasnâ€™t identified a Project Steward, you can report problems by emailing opensource@google.com.\nWe will investigate every complaint, but you may not receive a direct response. We will use our discretion in determining when and how to follow up on reported incidents, which may range from not taking action to permanent expulsion from the project and project-sponsored spaces. We will notify the accused of the report and provide them an opportunity to discuss it before any action is taken. The identity of the reporter will be omitted from the details of the report supplied to the accused. In potentially harmful situations, such as ongoing harassment or threats to anyone\u0026rsquo;s safety, we may take action without notice.\nThis document was adapted from the IndieWeb Code of Conduct.\n"},{"id":4,"href":"/modules/applied/","title":"Applied","parent":"Modules","content":"\nos tomllib DATA configuration_file Config Objects class Config() DEFAULT_CONFIG SECTION_DEFAULT SECTION_PROJECT SECTION_GCS SECTION_MODELS SECTION_VECTORS SECTION_BIG_QUERY SECTION_CATEGORY SECTION_TEST value @staticmethod def value(section: str = SECTION_DEFAULT, key: str = None) -\u0026gt; str | list[any] | int | float | bool | None __version__ {x-release-please-version}\npackage_version __version__ "},{"id":5,"href":"/modules/attributes/","title":"Attributes","parent":"Modules","content":"Functions related to attribute generation.\njson logging Optional m embeddings utils nearest_neighbors Config bq_client llm join_attributes_desc def join_attributes_desc(ids: list[str]) -\u0026gt; dict[str:dict] Gets the attributes and description for given product IDs.\nArgs: ids: The product IDs to get the attributes for.\nReturns dict mapping product IDs to attributes and descriptions. Each ID will map to a dict with the following keys: attributes: e.g. {\u0026lsquo;color\u0026rsquo;:\u0026lsquo;green\u0026rsquo;, \u0026lsquo;pattern\u0026rsquo;: striped} description: e.g. \u0026lsquo;This is a description\u0026rsquo;\nretrieve def retrieve(desc: str, category: Optional[str] = None, image: Optional[str] = None, base64: bool = False, filters: list[str] = []) -\u0026gt; list[dict] Returns list of attributes based on nearest neighbors.\nEmbeds the provided desc and (optionally) image and returns the attributes corresponding to the closest products in embedding space.\nArgs: desc: user provided description of product category: category of the product image: can be local file path, GCS URI or base64 encoded image base64: True indicates image is base64. False (default) will be interpreted as image path (either local or GCS) filters: category prefix to restrict results to\nReturns: List of candidates sorted by embedding distance. Each candidate is a dict with the following keys: id: product ID attributes: attributes in dict form e.g. {\u0026lsquo;color\u0026rsquo;:\u0026lsquo;green\u0026rsquo;, \u0026lsquo;pattern\u0026rsquo;: \u0026lsquo;striped\u0026rsquo;} description: string describing product distance: embedding distance in range [0,1], 0 being the closest match\ngenerate_prompt def generate_prompt(desc: str, candidates: list[dict]) -\u0026gt; str Populate LLM prompt template.\nArgs: desc: product description candidates: list of dicts with the following keys: attributes: attributes in dict form e.g. {\u0026lsquo;color\u0026rsquo;:\u0026lsquo;green\u0026rsquo;, \u0026lsquo;pattern\u0026rsquo;: \u0026lsquo;striped\u0026rsquo;} description: string describing product\nReturns: prompt to feed to LLM\nparse_answer def parse_answer(ans: str) -\u0026gt; dict[str, str] Translate LLM response into dict.\nArgs: ans: \u0026lsquo;|\u0026rsquo; separated key value pairs e.g. \u0026lsquo;color:red|size:large\u0026rsquo; Returns: ans as a dictionary\ngenerate_attributes def generate_attributes(desc: str, candidates: list[dict]) -\u0026gt; m.AttributeValue Use an LLM to determine attributes given nearest neighbor candidates\nArgs: desc: product description candidates: list of dicts with the following keys: attributes: attributes in dict form e.g. {\u0026lsquo;color\u0026rsquo;:\u0026lsquo;green\u0026rsquo;, \u0026lsquo;pattern\u0026rsquo;: \u0026lsquo;striped\u0026rsquo;} description: string describing product\nReturns: attributes in dict form e.g. {\u0026lsquo;color\u0026rsquo;:\u0026lsquo;green\u0026rsquo;, \u0026lsquo;pattern\u0026rsquo;: \u0026lsquo;striped\u0026rsquo;}\nretrieve_and_generate_attributes def retrieve_and_generate_attributes( desc: str, category: Optional[str] = None, image: Optional[str] = None, base64: bool = False, filters: list[str] = []) -\u0026gt; m.ProductAttributes RAG approach to generating product attributes.\nSince LLM answers are not always well formatted, if we fail to parse the LLM answer we fall back to a greedy retrieval approach.\nArgs: desc: user provided description of product category: category of the product image: can be local file path, GCS URI or base64 encoded image base64: True indicates image is base64. False (default) will be interpreted as image path (either local or GCS) num_neigbhors: number of nearest neighbors to return for EACH embedding filters: category prefix to restrict results to\nReturns: attributes in dict form e.g. {\u0026lsquo;color\u0026rsquo;:\u0026lsquo;green\u0026rsquo;, \u0026lsquo;pattern\u0026rsquo;: \u0026lsquo;striped\u0026rsquo;}\n"},{"id":6,"href":"/modules/categories/","title":"Categories","parent":"Modules","content":"Functions related to product categorization.\nlogging re defaultdict Optional m embeddings nearest_neighbors utils Config bq_client llm category_depth allow_trailing_nulls number_of_neighbors bq table_product column_id column_categories join_categories def join_categories(ids: list[str]) -\u0026gt; dict[str:list[str]] Given list of product IDs, join category names.\nArgs: ids: list of product IDs used to join against master product table\nReturns: dict mapping product IDs to category name. The category name will be a list of strings e.g. [\u0026rsquo;level 1 category\u0026rsquo;, \u0026rsquo;level 2 category']\nretrieve def retrieve(desc: str, image: Optional[str] = None, base64: bool = False, filters: list[str] = []) -\u0026gt; list[dict] Returns list of categories based on nearest neighbors.\nThis is a \u0026lsquo;greedy\u0026rsquo; retrieval approach that embeds the provided desc and (optionally) image and returns the categories corresponding to the closest products in embedding space.\nArgs: desc: user provided description of product image: can be local file path, GCS URI or base64 encoded image base64: True indicates image is base64. False (default) will be interpreted as image path (either local or GCS) filters: category prefix to restrict results to\nReturns: List of candidates sorted by embedding distance. Each candidate is a dict with the following keys: id: product ID category: category in list form e.g. [\u0026rsquo;level 1 category\u0026rsquo;, \u0026rsquo;level 2 category\u0026rsquo;] distance: embedding distance in range [0,1], 0 being the closest match\n_rank def _rank(desc: str, candidates: list[list[str]]) -\u0026gt; list[list[str]] See rank() for docstring.\nrank def rank(desc: str, candidates: list[list[str]]) -\u0026gt; list[list[str]] Use an LLM to rank candidates by description.\nArgs: desc: user provided description of product candidates: list of categories. Each category is in list form e.g. [\u0026rsquo;level 1 category\u0026rsquo;, \u0026rsquo;level 2 category\u0026rsquo;] so it\u0026rsquo;s a list of lists\nReturns: The candidates ranked by the LLM from most to least relevant. If there are duplicate candidates the list is deduped prior to returning\nretrieve_and_rank def retrieve_and_rank(desc: str, image: Optional[str] = None, base64: bool = False, filters: list[str] = []) -\u0026gt; m.CategoryList Wrapper function to sequence retrieve and rank functions.\nArgs: desc: user provided description of product image: can be local file path, GCS URI or base64 encoded image base64: True indicates image is base64. False (default) will be interpreted as image path (either local or GCS) num_neigbhors: number of nearest neighbors to return for EACH embedding filters: category prefix to restrict results to\nReturns: The candidates ranked by the LLM from most to least relevant. If there are duplicate candidates the list is deduped prior to returning\n"},{"id":7,"href":"/modules/embeddings/","title":"Embeddings","parent":"Modules","content":"\naiplatform_v1 embeddings utils Config index_client search_index_id insert_dp def insert_dp(dp_id: str, emb: list[float], cat=[]) upsert_dp def upsert_dp(prod_id: str, desc: str, image: str, cat=[]) remove_dp def remove_dp(dp_id: str) delete_dp def delete_dp(prod_id: str) Invoke Vertex Embedding API.\nlogging cache NamedTuple Optional Sequence aiplatform struct_pb2 Config EmbeddingResponse Objects class EmbeddingResponse(NamedTuple) text_embedding image_embedding EmbeddingPredictionClient Objects class EmbeddingPredictionClient() Wrapper around Prediction Service Client.\n__init__ def __init__() get_embedding def get_embedding(text: Optional[str] = None, image: Optional[str] = None, base64: bool = False) Invoke Vertex multimodal embedding API.\nYou can pass text and/or image. If neither is passed will raise exception\nArgs: text: text to embed image: can be local file path, GCS URI or base64 encoded image base64: True indicates image is base64. False (default) will be interpreted as image path (either local or GCS) Returns: named tuple with the following attributes: text_embedding: 1408 dimension vector of type Sequence[float] image_embedding: 1408 dimension vector of type Sequence[float] OR None if no image provide\nget_client @cache def get_client(project) embed def embed( text: str, image: Optional[str] = None, base64: bool = False, project: str = Config.value(\u0026#34;project\u0026#34;, \u0026#34;id\u0026#34;) ) -\u0026gt; EmbeddingResponse Invoke vertex multimodal embedding API.\nArgs: text: text to embed image: can be local file path, GCS URI or base64 encoded image base64: True indicates image is base64. False (default) will be interpreted as image path (either local or GCS) project: GCP Project ID\nReturns: named tuple with the following attributes: text_embedding: 1408 dimension vector of type Sequence[float] image_embedding: 1408 dimension vector of type Sequence[float] OR None if no image provide\n"},{"id":8,"href":"/modules/images/","title":"Images","parent":"Modules","content":"\nclient json typing request m utils Image Part multimodal_model get_image_bytes_from_url def get_image_bytes_from_url(image_url: str) -\u0026gt; bytes load_image_from_url def load_image_from_url(image_url: str) -\u0026gt; Image get_url_from_gcs def get_url_from_gcs(gcs_uri: str) -\u0026gt; str from_url def from_url(image: str) from_gsc_uri def from_gsc_uri(image_uri) content_generation def content_generation(prompt: str, im) image_to_attributes def image_to_attributes(req: m.ImageRequest) -\u0026gt; m.ProductAttributes image_to_product_description def image_to_product_description(image: str) -\u0026gt; m.TextValue "},{"id":9,"href":"/modules/marketing/","title":"Marketing","parent":"Modules","content":"Functions to generate marketing copy.\nm utils llm generate_marketing_copy def generate_marketing_copy(model: m.MarketingRequest) -\u0026gt; m.TextValue Given list of product IDs, join category names.\nArgs: model: Marketing Request\nReturns: Marketing copy that can be used for a product page\n"},{"id":10,"href":"/modules/model/","title":"Model","parent":"Modules","content":"\nOptional BaseModel CategoryList Objects class CategoryList(BaseModel) values Product Objects class Product(BaseModel) id description category image_uri ImageRequest Objects class ImageRequest(BaseModel) image AttributeValue Objects class AttributeValue(BaseModel) attribute_name attribute_value ProductAttributes Objects class ProductAttributes(BaseModel) product_attributes MarketingRequest Objects class MarketingRequest(BaseModel) description attributes TextValue Objects class TextValue(BaseModel) text Status Objects class Status(BaseModel) status Liveliness Objects class Liveliness(BaseModel) message parse_project_attributes_from_dict def parse_project_attributes_from_dict(obj: dict) -\u0026gt; ProductAttributes parse_list_to_dict def parse_list_to_dict(obj: list) -\u0026gt; dict attribute_values_to_dict def attribute_values_to_dict(values: list[AttributeValue]) -\u0026gt; dict[str, str] dict_to_attribute_values def dict_to_attribute_values(values: dict[str, str]) -\u0026gt; list[AttributeValue] "},{"id":11,"href":"/modules/nearest_neighbors/","title":"Nearest Neighbor","parent":"Modules","content":"Functions for Vertex Vector Search.\nlogging namedtuple aiplatform Namespace Config Neighbor index_endpoint deployed_index number_of_neighbors category_depth category_filter get_nn def get_nn(embeds: list[list[float]], filters: list[str] = [], num_neighbors: int = number_of_neighbors) -\u0026gt; list[Neighbor] Fetch nearest neighbors in vector store.\nNeighbors are fetched independently for each embedding then unioned.\nArgs: embeds: list of embeddings to find neareast neighbors filters: category prefix to restrict results to - example 1: [\u0026lsquo;Mens\u0026rsquo;] will only return suggestiongs with top level category \u0026lsquo;Mens\u0026rsquo; - example 2: [\u0026lsquo;Mens\u0026rsquo;, \u0026lsquo;Pants\u0026rsquo;] will only return suggestions with top level category \u0026lsquo;Mens\u0026rsquo; and second level category \u0026lsquo;Pants\u0026rsquo; num_neighbors: number of nearest neighbors to return for EACH embedding\nReturns: A list of named tuples containing the the following attributes id: unique item identifier, usually used to join to a reference DB distance: the embedding distance\n"},{"id":12,"href":"/modules/utilities/","title":"Utilities","parent":"Modules","content":"Functions common to several modules.\nvertexai preview cache Any aiplatform_v1 bigquery GenerativeModel Config conf get_bq_client @cache def get_bq_client(project=conf.value(Config.SECTION_PROJECT, \u0026#34;id\u0026#34;)) get_llm @cache def get_llm() get_gemini_pro_vision @cache def get_gemini_pro_vision() -\u0026gt; Any get_vector_search_index_client @cache def get_vector_search_index_client() "},{"id":13,"href":"/toolchain/","title":"Development Toolchain","parent":"Google Applied ML","content":" Data Scientists We strongly encourage you to follow the examples in the example directory. Use a standard virtual environment, finding all requirements in the conf/requirements.txt file.\nContributors Tooling This project uses Bazel to build all modules and manage dependencies. In conjunction with the Google Cloud CLI and app development libraries. Setting up a Bazel build environment is extremely simple, the following steps will get you up and running.\nDownload and install NPM (Node Package Manager) - NPM is used to manage Bazel through a plugin called Bazelisk. Once NPM is installed an your system path, open the console of your choice and issue the following command: npm install -g @bazel/bazelisk That\u0026rsquo;s it, bazel is ready to be used. Why Bazel with Python? Bazel is a hermetic build system, ensuring that when you build your software it WILL NOT be influenced by your local environment. Bazel supports many languages, Python is only one of them, and to build everything here, we also use Hugo and may use additional gRPC libraries. Bazel can operate along side your normal development tools and NOT impact your work life style. Bazel helps automate most developer tasks in a lightweight runtime. Bazel is configured using a language called Starlark, and it\u0026rsquo;s a lot like Python which makes it a natural fit. Building Building uses the following files:\nWORKSPACE - Defines the dependencies needed to build our software. BUILD.bazel - You\u0026rsquo;ll see these files in various directories, they tell bazel which software to build and test. Targets - Targets are defined in BUILD files by name, for example, docs/BUILD.bazel you can see \u0026ldquo;gen_docs\u0026rdquo;, this target builds the pydocs for the site, while other targets make them available as markdown in the final output directory. Targets can be access via a simple path //docs:gen_docs would tell bazel to perform that action. Rules - Are used to tell bazel what to do. For example: run, test, \u0026hellip; are types of rules, and every langauge has similar targets. Build Everything bazel build //... Build Some Things # Everything under docs bazel build //docs/... # All google libraries bazel build //google/... # Only build a single target # Because there is a rule named \u0026#39;applied\u0026#39; in this directory, it will run as the default rule bazel build //google/cloud/ml/applied # A single named target bazel build //docs:model Running a target # Run the document site locally bazel run //docs:serve # Start the service locally bazel run //examples/service:run Testing # Run all tests under the google directory bazel test //google/... "},{"id":14,"href":"/modules/","title":"Modules","parent":"Google Applied ML","content":" Introduction "},{"id":15,"href":"/","title":"Google Applied ML","parent":"","content":" Welcome "},{"id":16,"href":"/tags/","title":"Tags","parent":"Google Applied ML","content":""}]