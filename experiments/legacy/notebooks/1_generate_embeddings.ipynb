{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dy6Ma1jJIk-"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook demonstrates using multimodal [Vertex AI Embedding API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-image-embeddings) to enhance our data with embedded representations\n",
    "\n",
    "Input:\n",
    "- A BigQuery table with a text column and GCS URIs\n",
    "\n",
    "Output:\n",
    "- Two embeddings for each product stored in the BiqQuery table. \n",
    "- One based on the product descrption, and the other based on the first product image\n",
    "\n",
    "\n",
    "It is important to note that text and image data is not fused into a single embedding. The service provides text-only and image-only embeddings, but they share an embedding space. In other words the text ‘cat’ and a picture of a cat should return similar embeddings. This is useful because it allows us at inference time to operate with text-only or image-only inputs instead of requiring both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFZFdk1tJIk_"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FNPoyASex78"
   },
   "source": [
    "### Install Dependencies (If Needed)\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XP0gmJTkWQs9"
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p23YtY2ze8S_"
   },
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nF9O2bzVCzmg"
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cVyw7HafDZp"
   },
   "source": [
    "### Authenticate\n",
    "\n",
    "If you are using Colab, you will need to authenticate yourself first. The next cell will check if you are currently using Colab, and will start the authentication process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "liPfoHL8fDAZ"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzT0Vn6pfbJb"
   },
   "source": [
    "### Config\n",
    "\n",
    "Update the below variables to point to the BigQuery table and GCS bucket you created in the notebook 0_EDA_flipkart_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "XSgAq3jfJIk_"
   },
   "outputs": [],
   "source": [
    "PROJECT = 'solutions-2023-mar-107' # @param {type:\"string\"}\n",
    "LOCATION = 'us-central1' # @param {type:\"string\"}\n",
    "BQ_TABLE = 'solutions-2023-mar-107.flipkart.18K_no_duplicate' # @param {type:\"string\"}\n",
    "TEST_DESCRIPTION = \"Key Features of Vishudh Printed Women's Straight Kurta BLACK, GREY Straight,Specifications of Vishudh Printed Women's Straight Kurta Kurta Details Sleeve Sleeveless Number of Contents in Sales Package Pack of 1 Fabric 100% POLYESTER Type Straight Neck ROUND NECK General Details Pattern Printed Occasion Festive Ideal For Women's In the Box Kurta Additional Details Style Code VNKU004374 BLACK::GREY Fabric Care Gentle Machine Wash in Lukewarm Water, Do Not Bleach\" # @param {type:\"string\"}\n",
    "TEST_IMAGE = 'gs://genai-product-catalog/flipkart_20k_oct26/3ecb859759e5311cbab6850e98879522_0.jpg' # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kELokCl9JIk_"
   },
   "source": [
    "# Online Embedding API\n",
    "\n",
    "Client code for [multimodal embedding API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#api-usage), we reproduce it below with minor modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T_JW2BCAjVf7"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import struct_pb2\n",
    "from functools import cache\n",
    "import time\n",
    "import typing\n",
    "import logging\n",
    "\n",
    "\n",
    "class EmbeddingResponse(typing.NamedTuple):\n",
    "  text_embedding: typing.Sequence[float]\n",
    "  image_embedding: typing.Sequence[float]\n",
    "\n",
    "\n",
    "class EmbeddingPredictionClient:\n",
    "  \"\"\"Wrapper around Prediction Service Client.\"\"\"\n",
    "  def __init__(self, project : str,\n",
    "    location : str = \"us-central1\",\n",
    "    api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
    "    client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    self.location = location\n",
    "    self.project = project\n",
    "\n",
    "  def get_embedding(self, text : str = None, image_path : str = None):\n",
    "    \"\"\"image_path can be a local path or a GCS URI.\"\"\"\n",
    "    if not text and not image_path:\n",
    "      raise ValueError('At least one of text or image_bytes must be specified.')\n",
    "\n",
    "    instance = struct_pb2.Struct()\n",
    "\n",
    "    if text:\n",
    "      if len(text) > 1024:\n",
    "        logging.warning('Text must be less than 1024 characters. Truncating text.')\n",
    "        text = text[:1024]\n",
    "      instance.fields['text'].string_value = text\n",
    "\n",
    "    if image_path:\n",
    "      image_struct = instance.fields['image'].struct_value\n",
    "      if image_path.lower().startswith('gs://'):\n",
    "        image_struct.fields['gcsUri'].string_value = image_path\n",
    "      else:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "          image_bytes = f.read()\n",
    "        encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        image_struct.fields['bytesBase64Encoded'].string_value = encoded_content\n",
    "\n",
    "    instances = [instance]\n",
    "    endpoint = (f\"projects/{self.project}/locations/{self.location}\"\n",
    "      \"/publishers/google/models/multimodalembedding@001\")\n",
    "    try:\n",
    "      response = self.client.predict(endpoint=endpoint, instances=instances)\n",
    "\n",
    "      text_embedding = None\n",
    "      if text:\n",
    "        text_emb_value = response.predictions[0]['textEmbedding']\n",
    "        text_embedding = [v for v in text_emb_value]\n",
    "\n",
    "      image_embedding = None\n",
    "      if image_path:\n",
    "        image_emb_value = response.predictions[0]['imageEmbedding']\n",
    "        image_embedding = [v for v in image_emb_value]\n",
    "\n",
    "      return EmbeddingResponse(\n",
    "        text_embedding=text_embedding,\n",
    "        image_embedding=image_embedding)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      return None\n",
    "    \n",
    "@cache\n",
    "def get_client(project):\n",
    "  return EmbeddingPredictionClient(project)\n",
    "\n",
    "\n",
    "def embed(project,text,image_path=None):\n",
    "  client = get_client(project)\n",
    "  start = time.time()\n",
    "  response = client.get_embedding(text=text, image_path=image_path)\n",
    "  end = time.time()\n",
    "  print('Embedding Time: ', end - start)\n",
    "  return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79IuogmoJIk_"
   },
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "P83VI2tmJIk_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Time:  0.4698326587677002\n",
      "[-0.0165299866, -0.0692435578, 0.0147973252, 0.0349166617, 0.00536282221]\n",
      "[-0.00627771486, 0.0557949618, -0.0300531555, 0.0268286057, 0.0392316505]\n"
     ]
    }
   ],
   "source": [
    "res = embed(PROJECT, TEST_DESCRIPTION, TEST_IMAGE)\n",
    "print(res.text_embedding[:5])\n",
    "print(res.image_embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDvkPi3aJIlA"
   },
   "source": [
    "# Add Embeddings to BQ\n",
    "\n",
    "This is a naive approach which loads BQ rows sequentially and embeds using the online embedding API. We use pagination to avoid OOM errors for large datasets.\n",
    "\n",
    "We batch BQ row updates for efficiency. The embedding API is rate limited at 120 req/min so this is as fast as we can go.\n",
    "\n",
    "In the future, pending product updates, there may be more efficient options:\n",
    "1. Embed directly from BQ. This is supported for textembedding-gecko today (https://cloud.google.com/blog/products/data-analytics/introducing-bigquery-text-embeddings) and if support roles out for multimodal this would be the ideal choice\n",
    "2. Once a batch embedding API is available use that instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "y0x5J6PLJIlA"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(PROJECT)\n",
    "# Only fetch rows with no embedding. Bypass this query to update all rows\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  id,\n",
    "  description,\n",
    "  image_uri\n",
    "FROM\n",
    "  `{BQ_TABLE}`\n",
    "WHERE\n",
    "  ARRAY_LENGTH(text_embedding) = 0\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "query_job.result()\n",
    "destination = query_job.destination\n",
    "rows = client.list_rows(destination, max_results=1)\n",
    "print(rows.total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2oS2X5u9JIlA"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=10 #Set to 1 to clean up stragglers if rows not a multiple of batch_size\n",
    "text_embeddings, image_embeddings, ids = [], [], []\n",
    "\n",
    "for i,row in enumerate(rows):\n",
    "    print(f'\\n{i+1}: {row[\"description\"]}\\nimage_uri:{row[\"image_uri\"]}')\n",
    "    res = embed(PROJECT,row[\"description\"][:900],row[\"image_uri\"]) #API claims to supports up to 1024 chars but in practice get errors for shorter lengths\n",
    "    print(res.text_embedding[:5])\n",
    "    print(res.image_embedding[:5])\n",
    "    text_embeddings.append(res.text_embedding)\n",
    "    image_embeddings.append(res.image_embedding)\n",
    "    ids.append(row[\"id\"])\n",
    "    if len(text_embeddings) == BATCH_SIZE:\n",
    "      print(f'\\nBATCHING {BATCH_SIZE} UPDATES TO BQ...')\n",
    "      query = f\"\"\"\n",
    "      UPDATE\n",
    "        `{BQ_TABLE}`\n",
    "      SET\n",
    "        text_embedding = (\n",
    "          CASE\n",
    "            {''.join([f'WHEN id = \"{ids[i]}\" THEN {text_embeddings[i]}{chr(10)}' for i in range(len(ids))])}\n",
    "          END),\n",
    "        image_embedding = (\n",
    "          CASE\n",
    "            {''.join([f'WHEN id = \"{ids[i]}\" THEN {image_embeddings[i]}{chr(10)}' for i in range(len(ids))])}\n",
    "          END)\n",
    "      WHERE\n",
    "        id IN {str(ids).replace('[','(').replace(']',')')}\n",
    "      \"\"\"\n",
    "      start = time.time()\n",
    "      query_job = client.query(query)\n",
    "      query_job.result()  # Wait for the query to complete.\n",
    "      end = time.time()\n",
    "      print('BQ Update Time: ', end - start)\n",
    "      text_embeddings, image_embeddings, ids = [], [], []"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
